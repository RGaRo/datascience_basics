{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1253f8",
   "metadata": {},
   "source": [
    "# **Naive Bayes Explained with a Simple Spam Filter Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25175c76",
   "metadata": {},
   "source": [
    "### **Libraries used**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93c69a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "from math import log, exp\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from typing import List, Set, Dict, Iterable, Tuple, Literal\n",
    "from scratch.metrics import precision, recall, f1_score\n",
    "from scratch.data_preprocessing import Message, split_data, print_classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ae5a1",
   "metadata": {},
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb12ab82",
   "metadata": {},
   "source": [
    "**Bayes' Theorem and Its Role in Machine Learning**\n",
    "\n",
    "Bayes' Theorem is a fundamental concept in probability theory that provides a mathematical framework for updating beliefs in light of new evidence. Named after the 18th-century statistician Thomas Bayes, the theorem describes how to compute the probability of an event $A$ given that another event $B$ has occurred. Formally, it is expressed as:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $P(A|B)$ is the **posterior probability**: the updated probability of $A$ given that $B$ has occurred,\n",
    "- $P(B|A)$ is the **likelihood**: the probability of observing $B$ assuming $A$ is true,\n",
    "- $P(A)$ is the **prior probability**: our initial belief about $A$ before seeing $B$,\n",
    "- $P(B)$ is the **marginal probability**: the overall probability of observing $B$, regardless of $A$.\n",
    "\n",
    "In the next sections, we will explore the logic behind this formula.\n",
    "<br>\n",
    "\n",
    "**What You’ll Learn**\n",
    "\n",
    "In this notebook, we’ll walk through the implementation of a simple spam filter using the Naive Bayes model. Through this example, you’ll gain a deeper understanding of how Bayes’ Theorem works in practice and how probabilistic models can be applied to solve real-world classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d86f28",
   "metadata": {},
   "source": [
    "### **Theoretical Framework**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973ace1",
   "metadata": {},
   "source": [
    "**Bayes' Theorem in Machine Learning**\n",
    "\n",
    "In machine learning, Bayes' Theorem forms the foundation of a group of algorithms called **Bayesian classifiers**. One of the most popular and practical of these is the **Naive Bayes classifier**.\n",
    "\n",
    "The term *naive* refers to a simplifying assumption: it assumes that all features (e.g., words in an email) are conditionally independent given the class (e.g., spam or not spam). While this assumption is rarely true in real-world data, the model often works surprisingly well — especially in applications involving high-dimensional data like text.\n",
    "\n",
    "Naive Bayes is widely used in tasks such as:\n",
    "- Spam filtering,\n",
    "- Sentiment analysis,\n",
    "- Document classification.\n",
    "\n",
    "Its strengths include simplicity, interpretability, speed, and robustness — even with relatively small datasets.\n",
    "\n",
    "Let's see the maths behind the scenes.\n",
    "\n",
    "We start with the definition of conditional probability:\n",
    "\n",
    "$$\n",
    "\\tag{1}\n",
    "P(B|A) = \\frac{P(A \\cap B)}{P(A)}\n",
    "$$\n",
    "\n",
    "Equation (1) states that the probability of event $B$ occurring given event $A$ has occurred is the probability of both $A$ and $B$ happening together divided by the probability of $A$.\n",
    "\n",
    "Similarly, the conditional probability of $A$ given $B$ is:\n",
    "\n",
    "$$\n",
    "\\tag{2}\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$$\n",
    "\n",
    "Note that the joint probability $P(A \\cap B)$ appears in both equations (1) and (2). Rearranging equation (1) gives us:\n",
    "\n",
    "$$\n",
    "\\tag{3}\n",
    "P(A \\cap B) = P(B|A) \\cdot P(A)\n",
    "$$\n",
    "\n",
    "Substituting equation (3) into equation (2) yields Bayes Theorem:\n",
    "\n",
    "$$\n",
    "\\tag{4}\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This formula (4) allows us to **invert** the conditional probability: it expresses the probability of $A$ given $B$ in terms of the probability of $B$ given $A$, the prior probability of $A$, and the overall probability of $B$.\n",
    "\n",
    "Bayes' Theorem provides a powerful and elegant mathematical framework for reasoning about uncertainty and updating beliefs based on new evidence. Its ability to reverse conditional probabilities makes it an essential tool in machine learning, particularly for classification tasks.\n",
    "\n",
    "The Naive Bayes classifier builds on this theorem with a simplifying assumption of feature independence, which makes it computationally efficient and easy to implement. Despite its simplicity, it remains highly effective in many practical scenarios, especially in text-related problems such as spam filtering, where it leverages probabilistic reasoning to make informed predictions.\n",
    "\n",
    "Understanding this theoretical framework is crucial before diving into the implementation details and real-world applications.\n",
    "\n",
    "With the theory in place, let’s dive into a practical example to see it in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c587d8b",
   "metadata": {},
   "source": [
    "### **Practical exercise - A Spam Filter**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccdcf0d",
   "metadata": {},
   "source": [
    "To see how Bayes Theorem works in practice, let’s apply it to a simple spam filter scenario. Imagine we want to estimate the probability that an email is spam, given that it contains a specific word — also called a token — such as “free” or “discount”.\n",
    "\n",
    "In this context, we treat:\n",
    "- $A$ as the event “the email contains the token”, and\n",
    "- $B$ as the event “the email is spam”.\n",
    "\n",
    "Using Bayes’ Theorem, we can compute the probability that a message is spam given it contains the token, as follows:\n",
    "\n",
    "$$\n",
    "P(\\text{spam}|\\text{token}) = \\frac{P(\\text{token}|\\text{spam}) \\cdot P(\\text{spam})}{P(\\text{token})}\n",
    "$$\n",
    "\n",
    "Let’s break this down:\n",
    "- $P(\\text{spam}|\\text{token})$ is the posterior: the probability that the email is spam, given it contains the token.\n",
    "- $P(\\text{token}|\\text{spam})$ is the likelihood: how often this token appears in known spam emails.\n",
    "- $P(\\text{spam})$ is the prior: the overall probability that any email is spam, regardless of content.\n",
    "- $P(\\text{token})$ is the evidence: how frequently the token appears in all emails, spam or not.\n",
    "\n",
    "This equation gives us a systematic way to update our belief — how likely an email is spam — based on the presence of specific words. It forms the core of how probabilistic spam filters make decisions based on message content.\n",
    "\n",
    "The denominator, $P(\\text{token})$, represents the total probability of seeing the token in any email — spam or not.\n",
    "\n",
    "$$\n",
    "P(\\text{token}) = P(\\text{token}|\\text{spam}) \\cdot P(\\text{spam}) + P(\\text{token}|\\neg \\text{spam}) \\cdot P(\\neg \\text{spam})\n",
    "$$\n",
    "\n",
    "This expression accounts for two scenarios:\n",
    "1. The token appears in a spam email.\n",
    "2. The token appears in a non-spam (ham) email.\n",
    "\n",
    "Now that we understand each component, we can bring them all together in a complete formula. This gives us a practical expression to calculate the probability that an email is spam, given that it contains a specific token:\n",
    "\n",
    "$$\n",
    "\\tag{1}\n",
    "P(\\text{spam}|\\text{token}) = \\frac{P(\\text{token}|\\text{spam}) \\cdot P(\\text{spam})}{P(\\text{token}|\\text{spam}) \\cdot P(\\text{spam}) + P(\\text{token}|\\neg \\text{spam}) \\cdot P(\\neg \\text{spam})}\n",
    "$$\n",
    "\n",
    "Let’s assume we have no reason to believe that spam or non-spam messages are more frequent — in other words:\n",
    "\n",
    "$$\n",
    "\\tag{2}\n",
    "P(\\text{spam}) = P(\\neg \\text{spam}) = 0.5\n",
    "$$\n",
    "\n",
    "Substituting equation (2) into equation (1):\n",
    "\n",
    "$$\n",
    "\\tag{3}\n",
    "P(\\text{spam}|\\text{token}) = \\frac{P(\\text{token}|\\text{spam}) \\cdot 0.5}{P(\\text{token}|\\text{spam}) \\cdot 0.5 + P(\\text{token}|\\neg \\text{spam}) \\cdot 0.5}\n",
    "$$\n",
    "\n",
    "Now factor out the common prior:\n",
    "\n",
    "$$\n",
    "P(\\text{spam}|\\text{token}) = \\frac{0.5 \\cdot P(\\text{token}|\\text{spam})}{0.5 \\cdot \\left[P(\\text{token}|\\text{spam}) + P(\\text{token}|\\neg \\text{spam})\\right]}\n",
    "$$\n",
    "\n",
    "Cancel the 0.5 from numerator and denominator:\n",
    "\n",
    "$$\n",
    "\\tag{4}\n",
    "P(\\text{spam}|\\text{token}) = \\frac{P(\\text{token}|\\text{spam})}{P(\\text{token}|\\text{spam}) + P(\\text{token}|\\neg \\text{spam})}\n",
    "$$\n",
    "\n",
    "This simplified expression (4) allows us to estimate the probability that an email is spam just based on the relative likelihoods of the token appearing in spam vs. non-spam messages — assuming equal priors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd37be13",
   "metadata": {},
   "source": [
    "**Extending to Multiple Tokens**\n",
    "\n",
    "To handle an entire email, we consider the presence of multiple tokens (words). Assuming conditional independence between tokens — the core assumption of Naive Bayes — we can multiply Equation (4) for each token in the message:\n",
    "$$\n",
    "P(\\text{spam}|\\text{tokens}) =\n",
    "\\frac{\n",
    "\\prod_{i=1}^n P(\\text{token}_i|\\text{spam})\n",
    "}\n",
    "{\n",
    "\\prod_{i=1}^n P(\\text{token}_i|\\text{spam}) +\n",
    "\\prod_{i=1}^n P(\\text{token}_i|\\neg \\text{spam})\n",
    "}\n",
    "$$\n",
    "\n",
    "This formula estimates the probability that a message is spam given all its tokens. Each term in the product reflects how strongly a word supports the “spam” or “not spam” hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e99ca",
   "metadata": {},
   "source": [
    "**Handling Unseen Tokens**\n",
    "\n",
    "When a token has never appeared in the training data for a class (e.g., spam), we get:\n",
    "\n",
    "$$\n",
    "P(\\text{token}_i|\\text{spam}) = 0\n",
    "$$\n",
    "\n",
    "This would nullify the entire product. To avoid this, we apply a pseudocount smoothing:\n",
    "\n",
    "$$\n",
    "P(\\text{token}_i|\\text{spam}) = \\frac{\\text{count}(\\text{token}_i|\\text{spam}) + k}{\\text{total tokens in spam} + 2k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3a036",
   "metadata": {},
   "source": [
    "**Avoiding Underflow with Log Probabilities**\n",
    "\n",
    "When multiplying many small probabilities, the result can become extremely close to zero, causing numerical underflow — a problem where the computer cannot accurately represent very small numbers.\n",
    "\n",
    "To avoid this, we perform calculations in the logarithmic domain, using the property:\n",
    "\n",
    "$$\n",
    "\\log(ab) = \\log(a) + \\log(b)\n",
    "$$\n",
    "\n",
    "This means that instead of multiplying probabilities directly, we sum their logarithms, which is numerically more stable.\n",
    "\n",
    "Once we have the sum of the logs, we can convert back to the original probability by exponentiating:\n",
    "\n",
    "$$\n",
    "ab = \\exp\\big(\\log(ab)\\big)\n",
    "$$\n",
    "\n",
    "Now we start with the example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc88183",
   "metadata": {},
   "source": [
    "**Getting data**\n",
    "\n",
    "We will be using the corpus data from: https://spamassassin.apache.org/old/publiccorpus/, specifically the subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33610e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"datasets/spam_data/*/*\"\n",
    "data: List[Message] = []\n",
    "for filename in glob.glob(path):\n",
    "    is_spam = \"ham\" not in filename\n",
    "    with open(filename, errors='ignore') as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject:\"):\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append(Message(subject, is_spam))\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9a699",
   "metadata": {},
   "source": [
    "Now we will code a function for tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3748bfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'just', 'function', 'a', 'single', 'token', 'this', 'keep', 'lets', 'repeat', 'and'}\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text: str) -> Set[str]:\n",
    "    \"\"\"Reduce a text into a set of its words.\"\"\"\n",
    "    text = text.lower()\n",
    "    all_words = re.findall(\"[a-z0-9]+\", text)\n",
    "    return set(all_words)\n",
    "\n",
    "tokenize_example = tokenize('this function just keep a single token lets repeat this and function')\n",
    "print(tokenize_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1268891",
   "metadata": {},
   "source": [
    "And a class for our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa14e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesSpamClassifier:\n",
    "    \"\"\"\n",
    "    A model to predict if a message is spam where\n",
    "    k = smothing value\n",
    "    \"\"\"\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k\n",
    "        self.tokens = set()\n",
    "        self.token_spam_count: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_count: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = 0\n",
    "        self.ham_messages = 0\n",
    "    \n",
    "    def fit(self, messages: Iterable[Message]) -> None:\n",
    "        \"\"\"\n",
    "        Store the following:\n",
    "        spam_messages = count of spam messages\n",
    "        ham_messages = count of not spam messages\n",
    "        token_spam_count = count(token|spam) just one count per token per mail\n",
    "        token_ham_count = count(token|ham) just one count per token per mail\n",
    "        \"\"\"\n",
    "        for message in messages:\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            elif not message.is_spam:\n",
    "                self.ham_messages += 1\n",
    "        \n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_count[token] += 1\n",
    "                elif not message.is_spam:\n",
    "                    self.token_ham_count[token] += 1\n",
    "    \n",
    "    def _probabilities(self, token: str) -> Tuple[float, float, float, float]:\n",
    "        \"\"\"\n",
    "        Computes the probabilities listed below:\n",
    "        P(token|spam)\n",
    "        P(¬token|spam)\n",
    "        P(token|ham)\n",
    "        P(¬token|ham)\n",
    "        \"\"\"\n",
    "        p_token_given_spam = (self.k + self.token_spam_count[token])/ ((2*self.k) + (self.spam_messages))\n",
    "        p_not_token_given_spam = 1 - p_token_given_spam\n",
    "        p_token_given_ham = (self.k + self.token_ham_count[token])/ ((2*self.k) + (self.ham_messages))\n",
    "        p_not_token_given_ham = 1 - p_token_given_ham\n",
    "\n",
    "        return p_token_given_spam, p_not_token_given_spam, p_token_given_ham, p_not_token_given_ham\n",
    "    \n",
    "    def predict(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Return the probability of spam given the tokens.\n",
    "        \"\"\"\n",
    "        text_token = tokenize(text)\n",
    "        log_prob_token_given_spam = 0\n",
    "        log_prob_token_given_ham = 0\n",
    "\n",
    "        # Iterate through each token and add the log of the probability\n",
    "        for token in self.tokens:\n",
    "            p_token_given_spam, p_not_token_given_spam, p_token_given_ham, p_not_token_given_ham = self._probabilities(token)\n",
    "            \n",
    "            if token in text_token:\n",
    "                log_prob_token_given_spam += log(p_token_given_spam)\n",
    "                log_prob_token_given_ham += log(p_token_given_ham)\n",
    "            \n",
    "            elif token not in text_token:\n",
    "                log_prob_token_given_spam += log(p_not_token_given_spam)\n",
    "                log_prob_token_given_ham += log(p_not_token_given_ham)\n",
    "            \n",
    "        p_tokens_given_spam = exp(log_prob_token_given_spam)\n",
    "        p_tokens_given_ham = exp(log_prob_token_given_ham)\n",
    "\n",
    "        return p_tokens_given_spam/(p_tokens_given_spam + p_tokens_given_ham)\n",
    "    \n",
    "    def _cm(self, test_messages: List[Message], threshold: float = 0.5) -> Dict[str, float]:\n",
    "        \"\"\" Get the confusion matrix of each label\n",
    "            Cols - Predicted\n",
    "            Rows - Actual\n",
    "        \"\"\"\n",
    "        real_labels = [message.is_spam for message in test_messages]\n",
    "        predicted_labels = [self.predict(message.text)>threshold for message in test_messages]\n",
    "        labels = sorted(list(set(real_labels + predicted_labels)), reverse=True)\n",
    "        cm = confusion_matrix(real_labels, predicted_labels, labels=labels)\n",
    "        \n",
    "        cm_detailed = defaultdict(dict)\n",
    "        for label_index in range(len(labels)):\n",
    "            tp = 0\n",
    "            tn = 0\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            for row in range(len(labels)):\n",
    "                for col in range(len(labels)):\n",
    "                    if row==label_index and col==label_index:\n",
    "                        tp += int(cm[row][col])\n",
    "                    elif row==label_index and col!=label_index:\n",
    "                        fn += int(cm[row][col])\n",
    "                    elif row!=label_index and col==label_index:\n",
    "                        fp += int(cm[row][col])\n",
    "                    elif row!=label_index and col!=label_index:\n",
    "                        tn += int(cm[row][col])\n",
    "\n",
    "            cm_detailed[labels[label_index]]['tp'] = tp\n",
    "            cm_detailed[labels[label_index]]['tn'] = tn\n",
    "            cm_detailed[labels[label_index]]['fp'] = fp\n",
    "            cm_detailed[labels[label_index]]['fn'] = fn\n",
    "\n",
    "        return labels, cm, cm_detailed\n",
    "    \n",
    "    def metrics(self ,test_dataset: List[Message], \n",
    "                threshold: float = 0.5 ,\n",
    "                kind: Literal['micro','macro'] = 'micro') -> Dict[str, float]:\n",
    "        labels, cm, cm_detailed = self._cm(test_dataset, threshold = threshold)\n",
    "\n",
    "        if len(labels) == 2:\n",
    "            tp = cm_detailed[labels[0]]['tp']\n",
    "            fp = cm_detailed[labels[0]]['fp']\n",
    "            fn = cm_detailed[labels[0]]['fn']\n",
    "            tn = cm_detailed[labels[0]]['tn']\n",
    "            _accuracy = float(np.trace(cm)/np.sum(cm))\n",
    "            _precision = precision(tp, fp)\n",
    "            _recall = recall(tp, fn)\n",
    "            _f1_score = f1_score(tp, tn, fp, fn)\n",
    "            \n",
    "            return {\n",
    "                \"labels\": labels,\n",
    "                \"confusion_matrix\": cm,\n",
    "                \"confusion_matrix_detailes\": cm_detailed,\n",
    "                \"accuracy\": _accuracy, \n",
    "                \"precision\": _precision, \n",
    "                \"recall\": _recall,\n",
    "                \"f1_score\": _f1_score\n",
    "            }\n",
    "\n",
    "        elif len(labels) > 2:\n",
    "            _accuracy = float(np.trace(cm)/np.sum(cm))\n",
    "            if kind == 'micro':\n",
    "                tp = sum([cm_detailed[label]['tp'] for label in labels])\n",
    "                fp = sum([cm_detailed[label]['fp'] for label in labels])\n",
    "                fn = sum([cm_detailed[label]['fn'] for label in labels])\n",
    "                tn = sum([cm_detailed[label]['tn'] for label in labels])\n",
    "                _precision = precision(tp, fp)\n",
    "                _recall = recall(tp, fn)\n",
    "                _f1_score = f1_score(tp, tn, fp, fn)\n",
    "\n",
    "            elif kind == 'macro':\n",
    "                _precision = mean(\n",
    "                    [\n",
    "                        precision(\n",
    "                            cm_detailed[label]['tp'],\n",
    "                            cm_detailed[label]['fp']\n",
    "                        )\n",
    "                        for label in labels\n",
    "                    ]\n",
    "                \n",
    "                )\n",
    "\n",
    "                _recall = mean(\n",
    "                    [\n",
    "                        recall(\n",
    "                            cm_detailed[label]['tp'],\n",
    "                            cm_detailed[label]['fn']\n",
    "                        )\n",
    "                        for label in labels\n",
    "                    ]\n",
    "                \n",
    "                )\n",
    "\n",
    "                _f1_score = mean(\n",
    "                    [\n",
    "                        f1_score(\n",
    "                            cm_detailed[label]['tp'],\n",
    "                            cm_detailed[label]['tn'],\n",
    "                            cm_detailed[label]['fp'],\n",
    "                            cm_detailed[label]['fn']\n",
    "                        )\n",
    "                        for label in labels\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                raise AssertionError('Not a valid kind of metrics, use kind = \"micro\" or kind = \"macro\"')\n",
    "            \n",
    "            return {\n",
    "                \"labels\": labels,\n",
    "                \"confusion_matrix\": cm,\n",
    "                \"confusion_matrix_detailes\": cm_detailed,\n",
    "                \"accuracy\": _accuracy, \n",
    "                \"precision\": _precision, \n",
    "                \"recall\": _recall,\n",
    "                \"f1_score\": _f1_score\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe017c73",
   "metadata": {},
   "source": [
    "Training the model and making some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2e804f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Classification Report ===\n",
      "Labels: [True, False]\n",
      "\n",
      "Confusion Matrix:\n",
      "  85  54\n",
      "  22  664\n",
      "\n",
      "Detailed Confusion Metrics per Class:\n",
      "  1            -> TP: 85, TN: 664, FP: 22, FN: 54\n",
      "  0            -> TP: 664, TN: 85, FP: 54, FN: 22\n",
      "\n",
      "Overall Metrics:\n",
      "  Accuracy : 0.9079\n",
      "  Precision: 0.7944\n",
      "  Recall   : 0.6115\n",
      "  F1 Score : 0.6911\n"
     ]
    }
   ],
   "source": [
    "\n",
    "random.seed(0)\n",
    "train_messages, test_messages = split_data(data, .75)\n",
    "model = NaiveBayesSpamClassifier()\n",
    "model.fit(train_messages)\n",
    "metrics = model.metrics(test_messages)\n",
    "print_classification_report(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07377a5e",
   "metadata": {},
   "source": [
    "Just be aware, what we got of this prediction is the $P(spam|tokens)$ so you must defin a threshold for considering a prediction like a spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11ff82c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Re: Signer...\n",
      "Real Is Spam: False\n",
      "Predicted Is Spam: 5.124896331778981e-07\n",
      "Predicted Is Spam Labeled: False\n",
      "\n",
      "Subject: Patch to c...\n",
      "Real Is Spam: False\n",
      "Predicted Is Spam: 0.01862463789721551\n",
      "Predicted Is Spam Labeled: False\n",
      "\n",
      "Subject: RE: [Razor...\n",
      "Real Is Spam: False\n",
      "Predicted Is Spam: 5.983072599443672e-07\n",
      "Predicted Is Spam Labeled: False\n",
      "\n",
      "Subject: Fw: NORTON...\n",
      "Real Is Spam: True\n",
      "Predicted Is Spam: 0.9999995774247706\n",
      "Predicted Is Spam Labeled: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make 4 predictions\n",
    "predictions = [(f\"{message.text[:10]}...\", message.is_spam, model.predict(message.text)) for message in test_messages[:4]]\n",
    "\n",
    "for subject, real, predicted in predictions:\n",
    "    print(f\"Subject: {subject}\")\n",
    "    print(f\"Real Is Spam: {real}\")\n",
    "    print(f\"Predicted Is Spam: {predicted}\")\n",
    "    print(f\"Predicted Is Spam Labeled: {predicted>0.50}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c646bf11",
   "metadata": {},
   "source": [
    "### **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b78fef",
   "metadata": {},
   "source": [
    "Bayes Theorem helps us figure out the probability of something based on new information. In spam filtering, it lets us estimate how likely an email is spam if it contains certain words.\n",
    "\n",
    "The Naive Bayes model assumes that each word in an email is independent from the others, which makes calculations much easier and faster. Even though this assumption is simple, it works surprisingly well in practice.\n",
    "\n",
    "Thanks to this method, we can build a spam filter that uses math to make smart guesses — and it’s both fast and easy to implement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
