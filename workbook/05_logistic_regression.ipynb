{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d142ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Literal\n",
    "\n",
    "from scratch.complex_typing import Vector\n",
    "from scratch.metrics import precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05024cf3",
   "metadata": {},
   "source": [
    "Now we have see how many tools works from scratch we will use many sklearn implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8fb1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('datasets/titanic_dataset.csv')\n",
    "df_adjusted = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86536e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df):\n",
    "    \"\"\"\n",
    "    Prints a detailed summary of each column in a pandas DataFrame.\n",
    "    Shows type, missing values, and descriptive statistics for each column.\n",
    "    \"\"\"\n",
    "    print(f\"\\nDataFrame Summary: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for col in df.columns:\n",
    "        print(f\"\\nColumn: {col}\")\n",
    "        print(f\"Type: {df[col].dtype}\")\n",
    "        print(f\"Missing values: {df[col].isna().sum()}\")\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            print(f\"Mean: {df[col].mean():.5f}\")\n",
    "            print(f\"Median: {df[col].median():.5f}\")\n",
    "            print(f\"Min: {df[col].min():.5f}\")\n",
    "            print(f\"Max: {df[col].max():.5f}\")\n",
    "            print(f\"Standard Deviation: {df[col].std():.5f}\")\n",
    "        elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):\n",
    "            print(\"Value counts:\")\n",
    "            print(df[col].value_counts())\n",
    "        elif pd.api.types.is_bool_dtype(df[col]):\n",
    "            print(\"Value counts:\")\n",
    "            print(df[col].value_counts())\n",
    "        elif pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            print(f\"Min date: {df[col].min():.5f}\")\n",
    "            print(f\"Max date: {df[col].max():.5f}\")\n",
    "        else:\n",
    "            print(\"Unrecognized or complex data type.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\nEnd of summary.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee87e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame Summary: 891 rows, 12 columns\n",
      "============================================================\n",
      "\n",
      "Column: PassengerId\n",
      "Type: int64\n",
      "Missing values: 0\n",
      "Mean: 446.00000\n",
      "Median: 446.00000\n",
      "Min: 1.00000\n",
      "Max: 891.00000\n",
      "Standard Deviation: 257.35384\n",
      "\n",
      "Column: Survived\n",
      "Type: int64\n",
      "Missing values: 0\n",
      "Mean: 0.38384\n",
      "Median: 0.00000\n",
      "Min: 0.00000\n",
      "Max: 1.00000\n",
      "Standard Deviation: 0.48659\n",
      "\n",
      "Column: Pclass\n",
      "Type: int64\n",
      "Missing values: 0\n",
      "Mean: 2.30864\n",
      "Median: 3.00000\n",
      "Min: 1.00000\n",
      "Max: 3.00000\n",
      "Standard Deviation: 0.83607\n",
      "\n",
      "Column: Name\n",
      "Type: object\n",
      "Missing values: 0\n",
      "Value counts:\n",
      "Name\n",
      "Braund, Mr. Owen Harris                     1\n",
      "Boulos, Mr. Hanna                           1\n",
      "Frolicher-Stehli, Mr. Maxmillian            1\n",
      "Gilinski, Mr. Eliezer                       1\n",
      "Murdlin, Mr. Joseph                         1\n",
      "                                           ..\n",
      "Kelly, Miss. Anna Katherine \"Annie Kate\"    1\n",
      "McCoy, Mr. Bernard                          1\n",
      "Johnson, Mr. William Cahoone Jr             1\n",
      "Keane, Miss. Nora A                         1\n",
      "Dooley, Mr. Patrick                         1\n",
      "Name: count, Length: 891, dtype: int64\n",
      "\n",
      "Column: Sex\n",
      "Type: object\n",
      "Missing values: 0\n",
      "Value counts:\n",
      "Sex\n",
      "male      577\n",
      "female    314\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Column: Age\n",
      "Type: float64\n",
      "Missing values: 177\n",
      "Mean: 29.69912\n",
      "Median: 28.00000\n",
      "Min: 0.42000\n",
      "Max: 80.00000\n",
      "Standard Deviation: 14.52650\n",
      "\n",
      "Column: SibSp\n",
      "Type: int64\n",
      "Missing values: 0\n",
      "Mean: 0.52301\n",
      "Median: 0.00000\n",
      "Min: 0.00000\n",
      "Max: 8.00000\n",
      "Standard Deviation: 1.10274\n",
      "\n",
      "Column: Parch\n",
      "Type: int64\n",
      "Missing values: 0\n",
      "Mean: 0.38159\n",
      "Median: 0.00000\n",
      "Min: 0.00000\n",
      "Max: 6.00000\n",
      "Standard Deviation: 0.80606\n",
      "\n",
      "Column: Ticket\n",
      "Type: object\n",
      "Missing values: 0\n",
      "Value counts:\n",
      "Ticket\n",
      "347082      7\n",
      "CA. 2343    7\n",
      "1601        7\n",
      "3101295     6\n",
      "CA 2144     6\n",
      "           ..\n",
      "9234        1\n",
      "19988       1\n",
      "2693        1\n",
      "PC 17612    1\n",
      "370376      1\n",
      "Name: count, Length: 681, dtype: int64\n",
      "\n",
      "Column: Fare\n",
      "Type: float64\n",
      "Missing values: 0\n",
      "Mean: 32.20421\n",
      "Median: 14.45420\n",
      "Min: 0.00000\n",
      "Max: 512.32920\n",
      "Standard Deviation: 49.69343\n",
      "\n",
      "Column: Cabin\n",
      "Type: object\n",
      "Missing values: 687\n",
      "Unrecognized or complex data type.\n",
      "\n",
      "Column: Embarked\n",
      "Type: object\n",
      "Missing values: 2\n",
      "Unrecognized or complex data type.\n",
      "\n",
      "============================================================\n",
      "End of summary.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8f/z7k3sy6x0l555p2rqjf6zvtr0000gn/T/ipykernel_15289/604072689.py:20: DeprecationWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, pd.CategoricalDtype) instead\n",
      "  elif pd.api.types.is_string_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):\n"
     ]
    }
   ],
   "source": [
    "summary(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37799852",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_adjusted.columns = [col_name.strip().lower().replace(\" \",\"_\") for col_name in df_adjusted.columns]\n",
    "predictors_num = ['age', 'sibsp', 'parch','fare']\n",
    "predictors_cat = ['pclass', 'sex', 'embarked']\n",
    "target = 'survived'\n",
    "df_adjusted = df_adjusted[predictors_num + predictors_cat + [target]]\n",
    "\n",
    "train_data, test_data= train_test_split(df_adjusted, test_size=0.2, random_state=42)\n",
    "train_data_original = train_data.copy()\n",
    "test_data_original = test_data.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a0840",
   "metadata": {},
   "source": [
    "After this point we already have our data frame splitted into train and test datasets, we also need 2 more transofromations but we will do that over each dataset in different times:\n",
    "- Inputations\n",
    "- Get dummies\n",
    "- Normalization\n",
    "\n",
    "All this is important because we need an inputer and a StandardScaler for train dataset and we need ti apply exactly the same over the test dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3d520a",
   "metadata": {},
   "source": [
    "We need to inpute information for Age and Embarked we will do specific inputers for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a70b143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupByImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, group_cols, target_col, agg='mean'):\n",
    "        self.group_cols = group_cols\n",
    "        self.target_col = target_col\n",
    "        self.agg = agg\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill_values_ = (\n",
    "            X.groupby(self.group_cols)[self.target_col]\n",
    "            .agg(self.agg)\n",
    "            .to_dict()\n",
    "        )\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        def get_fill_value(row):\n",
    "            key = tuple(row[col] for col in self.group_cols)\n",
    "            return self.fill_values_.get(key, X[self.target_col].mean())\n",
    "        \n",
    "        mask = X[self.target_col].isnull()\n",
    "        X.loc[mask, self.target_col] = X[mask].apply(get_fill_value, axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8684726f",
   "metadata": {},
   "source": [
    "**Imputation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31532a54",
   "metadata": {},
   "source": [
    "*Train dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3f23b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the imputer we code before for getting ages considering agreggations in \"sex\" and \"pclass\"\n",
    "age_imputer = GroupByImputer(group_cols=['sex','pclass'], target_col='age', agg = 'mean')\n",
    "age_imputer.fit(train_data)\n",
    "train_data = age_imputer.transform(train_data)\n",
    "\n",
    "# Apply a knn imputer for \"embarked\" this is already available in sklearn modules\n",
    "# Input embarked\n",
    "col_to_inpute = 'embarked'\n",
    "le = LabelEncoder()\n",
    "not_nan_mask = train_data[col_to_inpute].notna()\n",
    "le.fit(train_data.loc[not_nan_mask, col_to_inpute])\n",
    "train_data.loc[not_nan_mask, col_to_inpute] = le.transform(train_data.loc[not_nan_mask, col_to_inpute])\n",
    "\n",
    "# Convert to float\n",
    "train_data[col_to_inpute] = train_data[col_to_inpute].astype(float)\n",
    "\n",
    "# Apply KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "columns = predictors_num + [col_to_inpute]\n",
    "knn_imputer.fit(train_data[columns])\n",
    "imputed = knn_imputer.transform(train_data[columns])\n",
    "\n",
    "# Replace the encoded column\n",
    "train_data[col_to_inpute] = imputed[:, columns.index(col_to_inpute)]\n",
    "\n",
    "# Decode back to original string labels\n",
    "train_data[col_to_inpute] = train_data[col_to_inpute].round().astype(int)\n",
    "train_data[col_to_inpute] = le.inverse_transform(train_data[col_to_inpute])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58592f7",
   "metadata": {},
   "source": [
    "*Test dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61df406b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the imputer we code before - We still using the one fitted in train dataset\n",
    "test_data = age_imputer.transform(test_data)\n",
    "\n",
    "# Apply a knn imputer for \"embarked\" this is already available in sklearn modules\n",
    "# Input embarked\n",
    "col_to_inpute = 'embarked'\n",
    "not_nan_mask = test_data[col_to_inpute].notna()\n",
    "test_data.loc[not_nan_mask, col_to_inpute] = le.transform(test_data.loc[not_nan_mask, col_to_inpute])\n",
    "\n",
    "# Convert to float\n",
    "test_data[col_to_inpute] = test_data[col_to_inpute].astype(float)\n",
    "\n",
    "# Apply KNN imputer\n",
    "columns = predictors_num + [col_to_inpute]\n",
    "imputed = knn_imputer.transform(test_data[columns])\n",
    "\n",
    "# Replace the encoded column\n",
    "test_data[col_to_inpute] = imputed[:, columns.index(col_to_inpute)]\n",
    "\n",
    "# Decode back to original string labels\n",
    "test_data[col_to_inpute] = test_data[col_to_inpute].round().astype(int)\n",
    "test_data[col_to_inpute] = le.inverse_transform(test_data[col_to_inpute])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4c9c5",
   "metadata": {},
   "source": [
    "**Dummies variables for categorical predictors**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3557eb72",
   "metadata": {},
   "source": [
    "*Train dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "433735cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8f/z7k3sy6x0l555p2rqjf6zvtr0000gn/T/ipykernel_15289/2462639932.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_data = train_data.replace({True: 1, False: 0})\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.get_dummies(train_data, columns=predictors_cat)\n",
    "train_data = train_data.replace({True: 1, False: 0})\n",
    "\n",
    "# Reordenar las columnas: target al final\n",
    "train_data = train_data[[c for c in train_data.columns if c != target] + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c7be8",
   "metadata": {},
   "source": [
    "*Test dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26b34fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8f/z7k3sy6x0l555p2rqjf6zvtr0000gn/T/ipykernel_15289/2626425710.py:3: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_data = test_data.replace({True: 1, False: 0})\n"
     ]
    }
   ],
   "source": [
    "# Get dummies\n",
    "test_data = pd.get_dummies(test_data, columns=predictors_cat)\n",
    "test_data = test_data.replace({True: 1, False: 0})\n",
    "\n",
    "# Reordenar las columnas: target al final\n",
    "test_data = test_data[[c for c in test_data.columns if c != target] + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd620a",
   "metadata": {},
   "source": [
    "**Normalization for numerical variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7ad98e",
   "metadata": {},
   "source": [
    "*Train dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "821a8749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to standardize\n",
    "cols_to_scale = predictors_num\n",
    "\n",
    "# Build transformer\n",
    "standar_scaler = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scaler', StandardScaler(), cols_to_scale)\n",
    "    ],\n",
    "    remainder='passthrough'  # keep other columns (e.g. 'gender') untouched\n",
    ")\n",
    "\n",
    "# Transform the data\n",
    "standar_scaler.fit(train_data)\n",
    "data_scaled = standar_scaler.transform(train_data)\n",
    "\n",
    "# Get correct column order and names\n",
    "new_columns = cols_to_scale + [col for col in train_data.columns if col not in cols_to_scale]\n",
    "train_data = pd.DataFrame(data_scaled, columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc5deb",
   "metadata": {},
   "source": [
    "*Test dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9e9f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalization - We still using the one fitted in train\n",
    "data_scaled = standar_scaler.transform(test_data)\n",
    "\n",
    "# Get correct column order and names\n",
    "new_columns = cols_to_scale + [col for col in test_data.columns if col not in cols_to_scale]\n",
    "test_data = pd.DataFrame(data_scaled, columns=new_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e620c96",
   "metadata": {},
   "source": [
    "**Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "685d6f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictors = [col for col in train_data.columns if col!='survived']\n",
    "final_target = 'survived'\n",
    "\n",
    "# Training data\n",
    "X_train = train_data[final_predictors]\n",
    "y_train = train_data[final_target]\n",
    "\n",
    "# Testing data\n",
    "X_test = test_data[final_predictors]\n",
    "y_test = test_data[final_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionScratch:\n",
    "    \"\"\" \n",
    "    A model to represent a logistic regression\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" \n",
    "        Initialize the attributes needed\n",
    "        \"\"\"\n",
    "        self.betas = []\n",
    "        self.learning_rate = None\n",
    "        self.epochs = None\n",
    "        \n",
    "    def fit(self,\n",
    "            X: List[Vector], \n",
    "            y: Vector, \n",
    "            learning_rate: float = 0.001, \n",
    "            epochs: int = 1000) -> None:\n",
    "        \"\"\" \n",
    "        A method to train the model\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.betas = [random.random() for _ in range(len(X[0]) + 1)]\n",
    "        pbar = tqdm.tqdm(range(self.epochs), \"Training model\")\n",
    "        for epoch in pbar:\n",
    "            gradient = np.mean([self._gradient([1] + xi, yi) for xi, yi in zip(X,y)], axis=0)\n",
    "            new_betas = self._gradient_step(gradient)\n",
    "            self.betas = new_betas\n",
    "            prob = self._prob(X, y)\n",
    "            pbar.set_postfix({\"prob\": f\"{prob:.4f}\"})\n",
    "    \n",
    "    def _prob(self, X: List[Vector], y: Vector) -> float:\n",
    "        \"\"\" \n",
    "        Computes the function result we are maximizing\n",
    "        \"\"\"\n",
    "        probabilities = []\n",
    "        for x_i,y_i in zip(X, y):\n",
    "            probability = (self._logistic([1] + x_i)**y_i)*((1 - self._logistic([1] + x_i))**(1-y_i))\n",
    "            probabilities.append(probability)\n",
    "        \n",
    "        return np.mean(probabilities).item()\n",
    "    \n",
    "    def _gradient_step(self, gradient: Vector)-> Vector:\n",
    "        \"\"\" \n",
    "        Make a step in the direction to maximize the function\n",
    "        \"\"\"\n",
    "        return [self.betas[i] + (self.learning_rate*gradient[i]) for i in range(len(self.betas))]\n",
    "    \n",
    "    def predict(self, x: Vector, threshold: float = 0.5) -> float:\n",
    "        \"\"\" \n",
    "        Method to predict\n",
    "        \"\"\"\n",
    "        z = np.dot(self.betas, [1] + x)\n",
    "        return 1 if 1/(1 + math.exp(-z)) >= threshold else 0\n",
    "    \n",
    "    def _logistic(self, x: Vector) -> float:\n",
    "        \"\"\" \n",
    "        Return f(x) -> logistic function\n",
    "        \"\"\"\n",
    "        z = np.dot(self.betas, x)\n",
    "        return 1/(1 + math.exp(-z))\n",
    "    \n",
    "    def _logistic_derivative(self, x:Vector) -> float:\n",
    "        \"\"\" \n",
    "        Returns the derivative of the logistic function\n",
    "        \"\"\"\n",
    "        return self._logistic(x) * (1 - self._logistic(x))\n",
    "    \n",
    "    def _gradient(self, x:Vector, y:float) -> Vector:\n",
    "        \"\"\" \n",
    "        Return the gradient of a point\n",
    "        The objective function is the probability\n",
    "        \"\"\"\n",
    "        gradient = [\n",
    "             (y - self._logistic(x))*x[i] for i in range(len(self.betas))\n",
    "        ]\n",
    "        return gradient\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b9a854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cm(self, test_dataset: List[LabeledPoint]) -> Dict[str, float]:\n",
    "    \"\"\" Get the confusion matrix of each label\n",
    "        Cols - Predicted\n",
    "        Rows - Actual\n",
    "    \"\"\"\n",
    "    real_labels = [lp.label for lp in test_dataset]\n",
    "    predicted_labels = [self.predict(lp.point) for lp in test_dataset]\n",
    "    labels = sorted(list(set(real_labels + predicted_labels)), reverse=True)\n",
    "    cm = confusion_matrix(real_labels, predicted_labels, labels=labels)\n",
    "    \n",
    "    cm_detailed = defaultdict(dict)\n",
    "    for label_index in range(len(labels)):\n",
    "        tp = 0\n",
    "        tn = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for row in range(len(labels)):\n",
    "            for col in range(len(labels)):\n",
    "                if row==label_index and col==label_index:\n",
    "                    tp += int(cm[row][col])\n",
    "                elif row==label_index and col!=label_index:\n",
    "                    fn += int(cm[row][col])\n",
    "                elif row!=label_index and col==label_index:\n",
    "                    fp += int(cm[row][col])\n",
    "                elif row!=label_index and col!=label_index:\n",
    "                    tn += int(cm[row][col])\n",
    "\n",
    "        cm_detailed[labels[label_index]]['tp'] = tp\n",
    "        cm_detailed[labels[label_index]]['tn'] = tn\n",
    "        cm_detailed[labels[label_index]]['fp'] = fp\n",
    "        cm_detailed[labels[label_index]]['fn'] = fn\n",
    "\n",
    "    return labels, cm, cm_detailed\n",
    "\n",
    "def metrics(self ,test_dataset: List[LabeledPoint], kind: Literal['micro','macro'] = 'micro') -> Dict[str, float]:\n",
    "    labels, cm, cm_detailed = self._cm(test_dataset)\n",
    "    # If just two labels we get the simpler confusion matrix and got metrics\n",
    "    if len(labels) == 2:\n",
    "        tp = cm_detailed[labels[0]]['tp']\n",
    "        fp = cm_detailed[labels[0]]['fp']\n",
    "        fn = cm_detailed[labels[0]]['fn']\n",
    "        tn = cm_detailed[labels[0]]['tn']\n",
    "        _accuracy = float(np.trace(cm)/np.sum(cm))\n",
    "        _precision = precision(tp, fp)\n",
    "        _recall = recall(tp, fn)\n",
    "        _f1_score = f1_score(tp, tn, fp, fn)\n",
    "\n",
    "        return {\n",
    "            \"labels\": labels,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"confusion_matrix_detailes\": cm_detailed,\n",
    "            \"accuracy\": _accuracy, \n",
    "            \"precision\": _precision, \n",
    "            \"recall\": _recall,\n",
    "            \"f1_score\": _f1_score\n",
    "        }\n",
    "    \n",
    "    # If multilabeled predictions we got micro or macro metrics\n",
    "    if len(labels) > 2:\n",
    "        _accuracy = float(np.trace(cm)/np.sum(cm))\n",
    "        if kind == 'micro':\n",
    "            tp = sum([cm_detailed[label]['tp'] for label in labels])\n",
    "            fp = sum([cm_detailed[label]['fp'] for label in labels])\n",
    "            fn = sum([cm_detailed[label]['fn'] for label in labels])\n",
    "            tn = sum([cm_detailed[label]['tn'] for label in labels])\n",
    "            _precision = precision(tp, fp)\n",
    "            _recall = recall(tp, fn)\n",
    "            _f1_score = f1_score(tp, tn, fp, fn)\n",
    "\n",
    "        elif kind == 'macro':\n",
    "            _precision = mean(\n",
    "                [\n",
    "                    precision(\n",
    "                        cm_detailed[label]['tp'],\n",
    "                        cm_detailed[label]['fp']\n",
    "                    )\n",
    "                    for label in labels\n",
    "                ]\n",
    "            \n",
    "            )\n",
    "\n",
    "            _recall = mean(\n",
    "                [\n",
    "                    recall(\n",
    "                        cm_detailed[label]['tp'],\n",
    "                        cm_detailed[label]['fn']\n",
    "                    )\n",
    "                    for label in labels\n",
    "                ]\n",
    "            \n",
    "            )\n",
    "\n",
    "            _f1_score = mean(\n",
    "                [\n",
    "                    f1_score(\n",
    "                        cm_detailed[label]['tp'],\n",
    "                        cm_detailed[label]['tn'],\n",
    "                        cm_detailed[label]['fp'],\n",
    "                        cm_detailed[label]['fn']\n",
    "                    )\n",
    "                    for label in labels\n",
    "                ]\n",
    "            \n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise AssertionError('Not a valid kind of metrics, use kind = \"micro\" or kind = \"macro\"')\n",
    "        \n",
    "        return {\n",
    "            \"labels\": labels,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"confusion_matrix_detailes\": cm_detailed,\n",
    "            \"accuracy\": _accuracy, \n",
    "            \"precision\": _precision, \n",
    "            \"recall\": _recall,\n",
    "            \"f1_score\": _f1_score\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c636a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model: 100%|██████████| 10000/10000 [03:51<00:00, 43.20it/s, prob=0.7151]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "model = LogisticRegressionScratch()\n",
    "model.fit(np.array(X_train).tolist(), np.array(y_train).tolist(), epochs=10000, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0276787c",
   "metadata": {},
   "source": [
    "Report in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "973a4291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.89      0.85       444\n",
      "         1.0       0.79      0.68      0.73       268\n",
      "\n",
      "    accuracy                           0.81       712\n",
      "   macro avg       0.80      0.78      0.79       712\n",
      "weighted avg       0.81      0.81      0.81       712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = [model.predict(x) for x in np.array(X_train).tolist()]\n",
    "print(\"Classification Report:\\n\", classification_report(np.array(y_train).tolist(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbab9d5b",
   "metadata": {},
   "source": [
    "Report in testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a062839e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.87      0.84       105\n",
      "         1.0       0.79      0.72      0.75        74\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.80      0.79      0.80       179\n",
      "weighted avg       0.80      0.80      0.80       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = [model.predict(x) for x in np.array(X_test).tolist()]\n",
    "print(\"Classification Report:\\n\", classification_report(np.array(y_test).tolist(), y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b51da5",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba9403",
   "metadata": {},
   "source": [
    "sklearn implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ceeeb2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.89      0.85       444\n",
      "         1.0       0.79      0.69      0.73       268\n",
      "\n",
      "    accuracy                           0.81       712\n",
      "   macro avg       0.81      0.79      0.79       712\n",
      "weighted avg       0.81      0.81      0.81       712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_train)\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20f14dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.88      0.88       105\n",
      "         1.0       0.82      0.82      0.82        74\n",
      "\n",
      "    accuracy                           0.85       179\n",
      "   macro avg       0.85      0.85      0.85       179\n",
      "weighted avg       0.85      0.85      0.85       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
